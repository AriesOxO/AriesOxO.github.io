---
author: meow
comments: true
title: 本地部署DeepSeek-R1模型
categories:
  - 后端
tags:
  - AI
  - DeepSeek
---

### [](https://www.wanghaoyu.com.cn/archives/Ollama-deepseek-r1.html#%E7%9B%AE%E5%BD%95 "目录")目录

\[TOC\]

首先 我们需要了解一下，什么是 Ollama ？

### [](https://www.wanghaoyu.com.cn/archives/Ollama-deepseek-r1.html#Ollama-%E7%AE%80%E4%BB%8B "Ollama 简介")Ollama 简介

> Ollama 是一个快速、轻量且易于使用的开源 AI 代理框架，由 Facebook 开发，可以用来托管和运行各种语言模型（LLM），可以实现在您的硬件设备快速部署各种本地大模型，并且提供了一个简单命令行界面，方便用户快速部署。

既然现在已经了解了 Ollama是干嘛的，那么接下来就是在你的操作系统中安装 Ollama了

### [](https://www.wanghaoyu.com.cn/archives/Ollama-deepseek-r1.html#%E5%AE%89%E8%A3%85Ollama "安装Ollama")安装Ollama

#### [](https://www.wanghaoyu.com.cn/archives/Ollama-deepseek-r1.html#Windows "Windows")Windows

访问 [Ollama官网](https://ollama.com/) 下载Windows版本安装即可

#### [](https://www.wanghaoyu.com.cn/archives/Ollama-deepseek-r1.html#Linux-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F "Linux 操作系统")Linux 操作系统

在你的终端中执行如下命令

<table><tbody><tr><td><pre><span>1</span><br></pre></td><td><pre><span>curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></tbody></table>

#### [](https://www.wanghaoyu.com.cn/archives/Ollama-deepseek-r1.html#Docker%E5%AE%89%E8%A3%85 "Docker安装")Docker安装

在 Linux操作系统中也可以通过 Docker进行安装 Ollama
如果您需要配置 Ollama 的一些选项（例如内存限制、日志级别等），可以在运行时指定：

<table><tbody><tr><td><pre><span>1</span><br></pre></td><td><pre><span>docker run -p 11434:11434 --name ollama \    -e <span>"OLLMARPC_LMAX_MEMORY=2GB"</span> \    -e <span>"OLLMARPC_LOG_LEVEL=INFO"</span> \    ollama/ollama:latest</span><br></pre></td></tr></tbody></table>

安装（启动完成之后） 可以通过命令行工具使用 Ollama 了

#### [](https://www.wanghaoyu.com.cn/archives/Ollama-deepseek-r1.html#%E6%B3%A8%E6%84%8F%EF%BC%9A "注意：")注意：

**默认 Ollama 会使用您的 CPU 来运行模型，而并非 GPU，对于那种比较小的模型用CPU+集成显卡也能较好的进行工作，如果您的计算机中有 AMD 或者 Nvidia 独立显卡，并且您想运行更大的模型或更快的响应速度，您需要安装CUDA Toolkit以更好地利用独立显卡**

##### [](https://www.wanghaoyu.com.cn/archives/Ollama-deepseek-r1.html#Ollama-%E6%94%AF%E6%8C%81%E7%9A%84-Nvidia-GPU "Ollama 支持的 Nvidia GPU")Ollama 支持的 Nvidia GPU

| Compute Capability | Family | Cards |
| --- | --- | --- |
| 9.0 | NVIDIA | `H100` |
| 8.9 | GeForce RTX 40xx | `RTX 4090` `RTX 4080` `RTX 4070 Ti` `RTX 4060 Ti` |
|  | NVIDIA Professional | `L4` `L40` `RTX 6000` |
| 8.6 | GeForce RTX 30xx | `RTX 3090 Ti` `RTX 3090` `RTX 3080 Ti` `RTX 3080` `RTX 3070 Ti` `RTX 3070` `RTX 3060 Ti` `RTX 3060` |
|  | NVIDIA Professional | `A40` `RTX A6000` `RTX A5000` `RTX A4000` `RTX A3000` `RTX A2000` `A10` `A16` `A2` |
| 8.0 | NVIDIA | `A100` `A30` |
| 7.5 | GeForce GTX/RTX | `GTX 1650 Ti` `TITAN RTX` `RTX 2080 Ti` `RTX 2080` `RTX 2070` `RTX 2060` |
|  | NVIDIA Professional | `T4` `RTX 5000` `RTX 4000` `RTX 3000` `T2000` `T1200` `T1000` `T600` `T500` |
|  | Quadro | `RTX 8000` `RTX 6000` `RTX 5000` `RTX 4000` |
| 7.0 | NVIDIA | `TITAN V` `V100` `Quadro GV100` |
| 6.1 | NVIDIA TITAN | `TITAN Xp` `TITAN X` |
|  | GeForce GTX | `GTX 1080 Ti` `GTX 1080` `GTX 1070 Ti` `GTX 1070` `GTX 1060` `GTX 1050` |
|  | Quadro | `P6000` `P5200` `P4200` `P3200` `P5000` `P4000` `P3000` `P2200` `P2000` `P1000` `P620` `P600` `P500` `P520` |
|  | Tesla | `P40` `P4` |
| 6.0 | NVIDIA | `Tesla P100` `Quadro GP100` |
| 5.2 | GeForce GTX | `GTX TITAN X` `GTX 980 Ti` `GTX 980` `GTX 970` `GTX 960` `GTX 950` |
|  | Quadro | `M6000 24GB` `M6000` `M5000` `M5500M` `M4000` `M2200` `M2000` `M620` |
|  | Tesla | `M60` `M40` |
| 5.0 | GeForce GTX | `GTX 750 Ti` `GTX 750` `NVS 810` |
|  | Quadro | `K2200` `K1200` `K620` `M1200` `M520` `M5000M` `M4000M` `M3000M` `M2000M` `M1000M` `K620M` `M600M` `M500M` |

##### [](https://www.wanghaoyu.com.cn/archives/Ollama-deepseek-r1.html#Ollama-%E6%94%AF%E6%8C%81%E7%9A%84-AMD-GPU "Ollama 支持的 AMD GPU")Ollama 支持的 AMD GPU

| Family | Cards and accelerators |
| --- | --- |
| AMD Radeon RX | `7900 XTX` `7900 XT` `7900 GRE` `7800 XT` `7700 XT` `7600 XT` `7600` `6950 XT` `6900 XTX` `6900XT` `6800 XT` `6800` `Vega 64` `Vega 56` |
| AMD Radeon PRO | `W7900` `W7800` `W7700` `W7600` `W7500` `W6900X` `W6800X Duo` `W6800X` `W6800` `V620` `V420` `V340` `V320` `Vega II Duo` `Vega II` `VII` `SSG` |
| AMD Instinct | `MI300X` `MI300A` `MI300` `MI250X` `MI250` `MI210` `MI200` `MI100` `MI60` `MI50` |

详情参考：[https://github.com/qianniucity/ollama-doc/blob/main/ollama/docs/Ollama%20%E5%AF%B9GPU%20%E6%94%AF%E6%8C%81%E4%BF%A1%E6%81%AF.md](https://github.com/qianniucity/ollama-doc/blob/main/ollama/docs/Ollama%20%E5%AF%B9GPU%20%E6%94%AF%E6%8C%81%E4%BF%A1%E6%81%AF.md)

### [](https://www.wanghaoyu.com.cn/archives/Ollama-deepseek-r1.html#Ollama-%E9%80%89%E6%8B%A9%E4%BD%BF%E7%94%A8-GPU-%E4%B8%8A%E8%BF%90%E8%A1%8C "Ollama 选择使用 GPU 上运行")Ollama 选择使用 GPU 上运行

如果希望 Ollama 使用 GPU 加速，需要确保已正确安装 GPU 驱动和相关库（如 CUDA 或 ROCm），然后可以通过以下方法指定 GPU，我这里有4张老黄家的 A10，所以我们要在启动之前配置好环境变量
例如

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br></pre></td><td><pre><span><span>export</span> CUDA_VISIBLE_DEVICES=0,1,2,3</span><br><span></span><br></pre></td></tr></tbody></table>

如果只有一张GPU 那么就是`export CUDA_VISIBLE_DEVICES=0`

如果是AMD CPU 参考这个,配置 `HIP_VISIBLE_DEVICES` 环境变量来指定要使用的 GPU,例如

<table><tbody><tr><td><pre><span>1</span><br></pre></td><td><pre><span>HIP_VISIBLE_DEVICES=0</span><br></pre></td></tr></tbody></table>

到这里 Ollama就基本配置完毕了，接下来就是运行模型

下面是 DeepSeek R1 模型的配置参考,以及运行方式，这里我用表格整理出来了

| **模型名称** | **模型大小** | **运行命令** | **硬件配置** |
| --- | --- | --- | --- |
| DeepSeek-R1 | 671B | `ollama run deepseek-r1:671b` | 需要极高的硬件配置，显存需求超过336GB |
| DeepSeek-R1-Distill-Qwen-1.5B | 1.5B | `ollama run deepseek-r1:1.5b` | 最低配置：8GB RAM，无显卡加速；适合老旧设备 |
| DeepSeek-R1-Distill-Qwen-7B | 7B | `ollama run deepseek-r1:7b` | 最低配置：16GB RAM，8GB显存（GPU加速） |
| DeepSeek-R1-Distill-Llama-8B | 8B | `ollama run deepseek-r1:8b` | 最低配置：16GB RAM，8GB显存（GPU加速） |
| DeepSeek-R1-Distill-Qwen-14B | 14B | `ollama run deepseek-r1:14b` | 最低配置：32GB RAM，26GB显存（GPU加速） |
| DeepSeek-R1-Distill-Qwen-32B | 32B | `ollama run deepseek-r1:32b` | 最低配置：64GB RAM，64GB显存（GPU加速） |
| DeepSeek-R1-Distill-Llama-70B | 70B | `ollama run deepseek-r1:70b` | 最低配置：128GB RAM，140GB显存（GPU加速） |

### [](https://www.wanghaoyu.com.cn/archives/Ollama-deepseek-r1.html#%E8%BF%90%E8%A1%8C-DeepSeek-R1-%E6%A8%A1%E5%9E%8B "运行 DeepSeek R1 模型")运行 DeepSeek R1 模型

根据上方表格的说明运行模型即可，例如，我用的是4张 A10 那么我完全可以跑 32B模型，可以通过`ollama run deepseek-r1:32b` 来启动该模型。

运行示例： ![deepseek-running](https://img.wanghaoyu.me/Picture/img20250201130609.png)

deepseek-running

### [](https://www.wanghaoyu.com.cn/archives/Ollama-deepseek-r1.html#%E4%BD%BF%E7%94%A8-API-%E8%B0%83%E7%94%A8-%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9E%8B "使用 API 调用 本地模型")使用 API 调用 本地模型

<table><tbody><tr><td><pre><span>1</span><br></pre></td><td><pre><span>curl -X POST http://localhost:11434/api/generate \ &nbsp; &nbsp; -H <span>"Content-Type: application/json"</span> \ &nbsp; &nbsp; -d <span>'{"prompt": "hello"}'</span></span><br></pre></td></tr></tbody></table>

会返回一个包含生成文本的 JSON 格式结果

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br></pre></td><td><pre><span><span>{</span></span><br><span>    <span>"response"</span><span>:</span> <span>"你好，我是 DeepSeek-R1。有什么我可以帮您的吗？"</span><span>,</span></span><br><span>    <span>"error"</span><span>:</span> <span><span>null</span></span></span><br><span><span>}</span></span><br></pre></td></tr></tbody></table>

参考链接：

-   [https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus)
-   [https://ollama.com/](https://ollama.com/)
-   [https://github.com/ollama/ollama/tree/main/docs](https://github.com/ollama/ollama/tree/main/docs)

原文链接：
- [本地部署 DeepSeek-R1 模型全攻略](https://www.wanghaoyu.com.cn/archives/Ollama-deepseek-r1.html)
